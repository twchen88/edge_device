{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f6964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import everything\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e90951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# [60000, 28, 28], [10000, 28, 28], [60000, ], [10000, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f864b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and organize the data\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_train = x_train.reshape(60000, 28 * 28)/255.0\n",
    "x_test = x_test.reshape(10000, 28 * 28)/255.0\n",
    "\n",
    "nClass = 10\n",
    "y_train = keras.utils.to_categorical(y_train, nClass)\n",
    "y_test = keras.utils.to_categorical(y_test, nClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model trained from main.ipynb\n",
    "\n",
    "model = keras.models.load_model('handwritten_model', compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to TF Lite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpp1mgmzve/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    }
   ],
   "source": [
    "# model conversion code taken from Tensorflow website\n",
    "\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "# check that input and output have been quantized to uint8 type\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675600"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the converted model\n",
    "\n",
    "import pathlib\n",
    "\n",
    "tflite_model_quant_file = pathlib.Path.cwd()/\"mnist_model_quant.tflite\"\n",
    "with open('tflite_model_quant_file', 'wb') as f:\n",
    "  f.write(tflite_model_quant)\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to run inference on a TFLite model\n",
    "\n",
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "  global x_test\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  for i, test_image_index in enumerate(test_image_indices):\n",
    "    test_image = x_test[test_image_index]\n",
    "    test_label = y_test[test_image_index]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    \n",
    "    # reshape test_image\n",
    "    test_image = tf.reshape(test_image, [1, 784])\n",
    "    \n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that gets the classification\n",
    "\n",
    "def getClass(lst):\n",
    "    return np.argmax(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to evaluate a TFLite model on all images\n",
    "\n",
    "def evaluate_model(tflite_file, model_type):\n",
    "  global x_test\n",
    "  global y_test\n",
    "\n",
    "  test_image_indices = range(x_test.shape[0])\n",
    "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
    "  tmp = [getClass(e) for e in y_test]\n",
    "\n",
    "  accuracy = (np.sum(tmp == predictions) * 100) / len(x_test)\n",
    "\n",
    "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "      model_type, accuracy, len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy is 97.8900% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model that was converted\n",
    "\n",
    "evaluate_model(tflite_model_quant_file, model_type=\"Quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy is 97.8900% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model that was saved\n",
    "\n",
    "evaluate_model(\"mnist_model_quant.tflite\", model_type=\"Quantized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
