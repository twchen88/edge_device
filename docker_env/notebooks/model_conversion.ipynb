{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f6964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import everything\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e90951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# [60000, 28, 28], [10000, 28, 28], [60000, ], [10000, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b45f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10)\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "dim = [(28 * 28,), 512, 512, 10]\n",
    "#input, first layer, second layer,output\n",
    "\n",
    "def makeModel(dim):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=dim[0]))\n",
    "    model.add(keras.layers.Dense(dim[1], activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(dim[2], activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(dim[3], activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "model = makeModel(dim)\n",
    "print(model.output_shape) # should be 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a63cb739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling\n",
    "\n",
    "optimizerMethod = \"adam\"\n",
    "lossFunction = \"categorical_crossentropy\"\n",
    "metricsLst = [\"accuracy\"]\n",
    "model.compile(optimizer=optimizerMethod, loss=lossFunction, metrics=metricsLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f864b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and organize the data\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_train = x_train.reshape(60000, 28 * 28)/255.0\n",
    "x_test = x_test.reshape(10000, 28 * 28)/255.0\n",
    "\n",
    "nClass = 10\n",
    "y_train = keras.utils.to_categorical(y_train, nClass)\n",
    "y_test = keras.utils.to_categorical(y_test, nClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730139d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 - 1s - loss: 0.2161 - accuracy: 0.9359 - val_loss: 0.1083 - val_accuracy: 0.9663\n",
      "Epoch 2/20\n",
      "469/469 - 1s - loss: 0.0787 - accuracy: 0.9758 - val_loss: 0.0719 - val_accuracy: 0.9770\n",
      "Epoch 3/20\n",
      "469/469 - 1s - loss: 0.0531 - accuracy: 0.9832 - val_loss: 0.0778 - val_accuracy: 0.9741\n",
      "Epoch 4/20\n",
      "469/469 - 1s - loss: 0.0357 - accuracy: 0.9885 - val_loss: 0.0807 - val_accuracy: 0.9755\n",
      "Epoch 5/20\n",
      "469/469 - 1s - loss: 0.0276 - accuracy: 0.9910 - val_loss: 0.0680 - val_accuracy: 0.9815\n",
      "Epoch 6/20\n",
      "469/469 - 1s - loss: 0.0224 - accuracy: 0.9925 - val_loss: 0.0798 - val_accuracy: 0.9781\n",
      "Epoch 7/20\n",
      "469/469 - 1s - loss: 0.0195 - accuracy: 0.9931 - val_loss: 0.0788 - val_accuracy: 0.9773\n",
      "Epoch 8/20\n",
      "469/469 - 1s - loss: 0.0155 - accuracy: 0.9947 - val_loss: 0.0782 - val_accuracy: 0.9804\n",
      "Epoch 9/20\n",
      "469/469 - 1s - loss: 0.0163 - accuracy: 0.9946 - val_loss: 0.0901 - val_accuracy: 0.9779\n",
      "Epoch 10/20\n",
      "469/469 - 1s - loss: 0.0160 - accuracy: 0.9945 - val_loss: 0.0840 - val_accuracy: 0.9798\n",
      "Epoch 11/20\n",
      "469/469 - 1s - loss: 0.0125 - accuracy: 0.9957 - val_loss: 0.0899 - val_accuracy: 0.9807\n",
      "Epoch 12/20\n",
      "469/469 - 1s - loss: 0.0086 - accuracy: 0.9969 - val_loss: 0.0988 - val_accuracy: 0.9787\n",
      "Epoch 13/20\n",
      "469/469 - 1s - loss: 0.0095 - accuracy: 0.9970 - val_loss: 0.0857 - val_accuracy: 0.9821\n",
      "Epoch 14/20\n",
      "469/469 - 1s - loss: 0.0095 - accuracy: 0.9968 - val_loss: 0.0968 - val_accuracy: 0.9795\n",
      "Epoch 15/20\n",
      "469/469 - 1s - loss: 0.0122 - accuracy: 0.9959 - val_loss: 0.0959 - val_accuracy: 0.9784\n",
      "Epoch 16/20\n",
      "469/469 - 1s - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.0968 - val_accuracy: 0.9804\n",
      "Epoch 17/20\n",
      "469/469 - 1s - loss: 0.0092 - accuracy: 0.9971 - val_loss: 0.0973 - val_accuracy: 0.9806\n",
      "Epoch 18/20\n",
      "469/469 - 1s - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0905 - val_accuracy: 0.9833\n",
      "Epoch 19/20\n",
      "469/469 - 1s - loss: 0.0058 - accuracy: 0.9979 - val_loss: 0.0934 - val_accuracy: 0.9826\n",
      "Epoch 20/20\n",
      "469/469 - 1s - loss: 0.0100 - accuracy: 0.9971 - val_loss: 0.1386 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f22842dac18>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=2, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ecd90f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.1386 - accuracy: 0.9752\n",
      "Loss:  0.13864773511886597\n",
      "Accuracy:  0.9751999974250793\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "results = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Loss: \", results[0])\n",
    "print(\"Accuracy: \", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# model = tf.keras.applications.mobilenet.MobileNet(input_shape=(1, 28 * 28, 3), weights=None)\n",
    "# model.save('handwritten_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to use TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp39mxuwvh/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1dppavge/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1dppavge/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pathlib\n",
    "\n",
    "# tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
    "# tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# # Save the unquantized/float model:\n",
    "# tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "# tflite_model_file.write_bytes(tflite_model)\n",
    "# # Save the quantized model:\n",
    "# tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
    "# tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
    "# tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the unquantized/float model:\n",
    "# tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "# tflite_model_file.write_bytes(tflite_model)\n",
    "# Save the quantized model:\n",
    "tflite_model_quant_file = pathlib.Path.cwd()/\"mnist_model_quant.tflite\"\n",
    "# with open('tflite_model_quant_file', 'wb') as f:\n",
    "#   f.write(tflite_model_quant)\n",
    "# tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "  global x_test\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  for i, test_image_index in enumerate(test_image_indices):\n",
    "    test_image = x_test[test_image_index]\n",
    "    test_label = y_test[test_image_index]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    \n",
    "    # reshape test_image\n",
    "    test_image = tf.reshape(test_image, [1, 784])\n",
    "    \n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Change this to test a different image\n",
    "test_image_index = 1\n",
    "\n",
    "## Helper function to test the models on one image\n",
    "def test_model(tflite_file, test_image_index, model_type):\n",
    "  global y_test\n",
    "\n",
    "  predictions = run_tflite_model(tflite_file, [test_image_index])\n",
    "\n",
    "  tmp = tf.reshape(x_test[test_image_index], [28, 28])\n",
    "  plt.imshow(tmp)\n",
    "  template = model_type + \" Model \\n True:{true}, Predicted:{predict}\"\n",
    "  _ = plt.title(template.format(true= str(y_test[test_image_index]), predict=str(predictions[0])))\n",
    "  plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_model(tflite_model_quant_file, test_image_index, model_type=\"Quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClass(lst):\n",
    "    return np.argmax(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate a TFLite model on all images\n",
    "def evaluate_model(tflite_file, model_type):\n",
    "  global x_test\n",
    "  global y_test\n",
    "\n",
    "  test_image_indices = range(x_test.shape[0])\n",
    "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
    "  tmp = [getClass(e) for e in y_test]\n",
    "\n",
    "  accuracy = (np.sum(tmp == predictions) * 100) / len(x_test)\n",
    "\n",
    "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "      model_type, accuracy, len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy is 98.5700% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_quant_file, model_type=\"Quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy is 98.5700% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(\"mnist_model_quant.tflite\", model_type=\"Quantized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
